{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from surprise import Dataset, Reader, dump\n",
    "from surprise import BaselineOnly, KNNBasic, NMF, SVD\n",
    "from surprise.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User-ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Age</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>657681</th>\n",
       "      <td>162297</td>\n",
       "      <td>n/a, n/a, australia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0671738704</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055990</th>\n",
       "      <td>254467</td>\n",
       "      <td>walnut creek, california, usa</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0380820293</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21136</th>\n",
       "      <td>7158</td>\n",
       "      <td>omaha, nebraska, usa</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0679419853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556638</th>\n",
       "      <td>136010</td>\n",
       "      <td>brampton, ontario, canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0395589681</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502190</th>\n",
       "      <td>124091</td>\n",
       "      <td>st. louis, missouri, usa</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0345417097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         User-ID                       Location   Age        ISBN  Book-Rating\n",
       "657681    162297            n/a, n/a, australia   NaN  0671738704            5\n",
       "1055990   254467  walnut creek, california, usa  29.0  0380820293            7\n",
       "21136       7158           omaha, nebraska, usa  30.0  0679419853            0\n",
       "556638    136010      brampton, ontario, canada   NaN  0395589681            0\n",
       "502190    124091       st. louis, missouri, usa  21.0  0345417097            0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv('./Users.csv')\n",
    "ratings = pd.read_csv('./Ratings.csv')\n",
    "\n",
    "dataset = pd.merge(users, ratings, on='User-ID', how='inner')\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dimensionality and prevent memory errors : remove rarely rated books and rarely rating users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data frame shape:\t(1149780, 5)\n",
      "The new data frame shape:\t(282829, 5)\n"
     ]
    }
   ],
   "source": [
    "min_book_ratings = 20\n",
    "filter_books = dataset['ISBN'].value_counts() > min_book_ratings\n",
    "filter_books = filter_books[filter_books].index.tolist()\n",
    "\n",
    "min_user_ratings = 20\n",
    "filter_users = dataset['User-ID'].value_counts() > min_user_ratings\n",
    "filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "dataset_filtered = dataset[(dataset['ISBN'].isin(filter_books)) & (dataset['User-ID'].isin(filter_users))]\n",
    "print('The original data frame shape:\\t{}'.format(dataset.shape))\n",
    "print('The new data frame shape:\\t{}'.format(dataset_filtered.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modeling is done with the <b>Surprise</b> library which is <em>a scikit for building and analyzing recommender systems with explicit rating data</em>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the surprise Dataset\n",
    "reader = Reader(rating_scale=(0, 9))\n",
    "data = Dataset.load_from_df(dataset_filtered[['User-ID', 'ISBN', 'Book-Rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split Data\n",
    "# Split the data into train and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Instantiate Algorithms\n",
    "baseline_algo = BaselineOnly()\n",
    "knn_algo = KNNBasic()\n",
    "nmf_algo = NMF()\n",
    "svd_algo = SVD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fcc7d3455e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Hyperparameter Grids for Each Algorithm\n",
    "param_grid_baseline = {\n",
    "    'bsl_options': {\n",
    "        'method': ['als', 'sgd'],\n",
    "        'reg': [0.02, 0.05, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "param_grid_knn = {\n",
    "    'k': [10, 20, 30],\n",
    "    'min_k': [1, 3, 5]\n",
    "}\n",
    "\n",
    "param_grid_nmf = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'n_epochs': [20, 50, 100]\n",
    "}\n",
    "\n",
    "param_grid_svd = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'n_epochs': [20, 50, 100],\n",
    "    'lr_all': [0.002, 0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Perform Grid Search with Cross-validation for Each Algorithm\n",
    "grid_search_baseline = GridSearchCV(BaselineOnly, param_grid_baseline, measures=['rmse'], cv=3)\n",
    "grid_search_baseline.fit(trainset)\n",
    "\n",
    "grid_search_knn = GridSearchCV(KNNBasic, param_grid_knn, measures=['rmse'], cv=3)\n",
    "grid_search_knn.fit(trainset)\n",
    "\n",
    "grid_search_nmf = GridSearchCV(NMF, param_grid_nmf, measures=['rmse'], cv=3)\n",
    "grid_search_nmf.fit(trainset)\n",
    "\n",
    "grid_search_svd = GridSearchCV(SVD, param_grid_svd, measures=['rmse'], cv=3)\n",
    "grid_search_svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid='242', iid='0060977493', r_ui=None, est=2.270259253997794, details={'was_impossible': False})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Get the Best Hyperparameters and RMSE for Each Algorithm\n",
    "best_params_baseline = grid_search_baseline.best_params['rmse']\n",
    "best_rmse_baseline = grid_search_baseline.best_score['rmse']\n",
    "\n",
    "best_params_knn = grid_search_knn.best_params['rmse']\n",
    "best_rmse_knn = grid_search_knn.best_score['rmse']\n",
    "\n",
    "best_params_nmf = grid_search_nmf.best_params['rmse']\n",
    "best_rmse_nmf = grid_search_nmf.best_score['rmse']\n",
    "\n",
    "best_params_svd = grid_search_svd.best_params['rmse']\n",
    "best_rmse_svd = grid_search_svd.best_score['rmse']\n",
    "\n",
    "print(\"Best Hyperparameters for BaselineOnly:\", best_params_baseline)\n",
    "print(\"Best RMSE for BaselineOnly:\", best_rmse_baseline)\n",
    "\n",
    "print(\"Best Hyperparameters for KNNBasic:\", best_params_knn)\n",
    "print(\"Best RMSE for KNNBasic:\", best_rmse_knn)\n",
    "\n",
    "print(\"Best Hyperparameters for NMF:\", best_params_nmf)\n",
    "print(\"Best RMSE for NMF:\", best_rmse_nmf)\n",
    "\n",
    "print(\"Best Hyperparameters for SVD:\", best_params_svd)\n",
    "print(\"Best RMSE for SVD:\", best_rmse_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train Final Models with Selected Hyperparameters on Combined Train and Validation Sets\n",
    "best_baseline_algo = BaselineOnly(**best_params_baseline)\n",
    "best_knn_algo = KNNBasic(**best_params_knn)\n",
    "best_nmf_algo = NMF(**best_params_nmf)\n",
    "best_svd_algo = SVD(**best_params_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which model is the best ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Precision@k and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        # Number of relevant items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "\n",
    "        # Number of recommended items in top k\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(\n",
    "            ((true_r >= threshold) and (est >= threshold))\n",
    "            for (est, true_r) in user_ratings[:k]\n",
    "        )\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
    "\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
    "\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
    "\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Precision@K and Recall@K for each model\n",
    "# and keep the best.\n",
    "\n",
    "best_models = {'baseline' : best_baseline_algo,\n",
    "               'knn' : best_knn_algo,\n",
    "               'nmf' : best_nmf_algo,\n",
    "               'svd' : best_svd_algo\n",
    "               }\n",
    "\n",
    "results = defaultdict()\n",
    "for name, algo in best_models.items():\n",
    "\n",
    "    predictions = algo.test(testset)\n",
    "    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=4)\n",
    "\n",
    "    average_precision = sum(prec for prec in precisions.values()) / len(precisions)\n",
    "    average_recall = sum(rec for rec in recalls.values()) / len(recalls)\n",
    "\n",
    "    results[name] = {'precision@k' : average_precision,\n",
    "                     'recall@k' : average_recall}\n",
    "\n",
    "    print(f'''Precision {name} : {average_precision}\n",
    "           Recall {name} : {average_recall}\n",
    "           ''')\n",
    "\n",
    "best_by_precision = results.sort(key=lambda x: x['precision@k'], reverse=True)\n",
    "best_by_recall = results.sort(key=lambda x: x['recall@k'], reverse=True)\n",
    "\n",
    "best_by_precision[0]\n",
    "best_by_recall[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Top 10 recommandations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = grid_search_baseline.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump algorithm and reload it.\n",
    "file_name = os.path.expanduser(\"~/dump_file\")\n",
    "dump.dump(file_name, algo=algo)\n",
    "_, loaded_algo = dump.load(file_name)\n",
    "\n",
    "# We now ensure that the algo is still the same by checking the predictions.\n",
    "predictions_loaded_algo = loaded_algo.test(trainset.build_testset())\n",
    "assert predictions == predictions_loaded_algo\n",
    "print(\"Predictions are the same\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book-recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
